{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "The following code was designed in order to track the location of a single animal across the course of a single video session.  After initally loading in the video, the user is able to crop the video frame using a rectangle selection tool.  A background reference frame is then specified, either by taking a median of several frames in the video, or by the user providing a short video of the same environment without an animal in it.  By comparing each frame in the video to the reference frame, the location of the animal can be tracked.  It is imperative that the reference frame of the video is not shifted from the actual video.  For this reason, if a separate video is supplied, it is best that it be acquired on the same day as the behavioral recording.  The animal's center of mass, in x,y coordinates, is then recorded, as well as the distance in pixels that the animal moves from one frame to the next. Lastly, the user can specify regions of interest in the frame (e.g. left, right) using a polygon drawing tool and record for each frame whether the animal is in the region of interest.  Options for summarizing the data are also provided. \n",
    "\n",
    "### Package Requirements\n",
    "Please see instructions under repository README for package requirements and install instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Load Necessary Packages\n",
    "The following code loads neccessary packages and need not be changed by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import LocationTracking_Functions as lt\n",
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. User Sets Directory and File Information, and Specifies ROI Names, if any.\n",
    "Below the user sets the directory file path (path of the folder where the video file is), the file name of the video to be analyzed, the frame on which the analysis is to begin (0 is the first frame), the last frame to be analyzed (set `'end' : None` if processing entire video), and the degree to which the video should be spatially downsampled (0-1.  E.g, 0.25 will reduce frame to 1/4 it's originally size; 1=no downsampling).  Additionally, the user defines the names of any regions of interest.  Any number of regions of interst can be used.  If no regions of interest are to be used, set `region_names = None`. By modifying 'stretch', one can alter the relative width/height of the presented output.\n",
    "\n",
    "***Processing going slow?  Consider downsampling!***  Often times tracking does not not require 1080p or whatever high def resolution videos are sometimes acquired using. Try setting `dsmpl` to something lower than 1 to implement downsampling (1=no downsampling).  For example, if set to 0.25, each frame will be downsampled to 1/4 the original size\n",
    "\n",
    "***Windows Users:*** Place an 'r' in front directory path (e.g. r\"zp\\Videos\") to avoid mishandling of forward slashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict = {\n",
    "    'dpath' : \"/Users/me/Videos/\",  \n",
    "    'file' : 'video.mpg',\n",
    "    'start' : 0, \n",
    "    'end' : None,\n",
    "    'dsmpl' : 1\n",
    "}\n",
    "stretch = dict(width=1, height=1)\n",
    "\n",
    "region_names = None #['Left','Right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Load Video and Crop Frame if Desired\n",
    "To crop video frame, after running code below, select box selection tool below image (square with a plus sign).  To start drawing region to be included in analyis, double click image.  Double click again to finalize region.  If you decide to change region, it is best to rerun this cell and subsequent steps.\n",
    "\n",
    "If the size of the image is too small/large, alter the first line of code.  100 is the standard size.  200 will produce an image 2x the size, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "image,crop,video_dict = lt.LoadAndCrop(video_dict,stretch,cropmethod='Box')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. (Optional) Mask Internal Regions\n",
    "\n",
    "The following code is used to exclude internal portion/s of the field of view from the analysis. After running cell below, draw regions to be excluded.  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "plot,poly_stream,video_dict['mask'] = lt.Mask_select(video_dict,stretch,crop=crop)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Define Reference Frame for Location Tracking.\n",
    "For location tracking to work, view of box without animal must be provided.  Below there are two ways to do this.  **Option 1** provides a method to remove the animal from the video.  This option works well provided the animal doesn't stay in the same location for >50% of the session. Alternatively, with **Option 2**, the user can simply define a video file in the same folder that doesn't have in animal in it.  Option 1 is generally recormmended. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Create reference frame by removing animal from video\n",
    "The following code takes 100 random frames across the session and creates an average of them by taking the median for each pixel.  This will remove influence of animal on any given frame.  The number of random frames used can be changed with the variable `num_frames`.  Alternatively, if the user would like to manually define the frames to be used, `frames` can be set to a list frame numbers: e.g. `frames = np.arange(100,500,5`, which would select every 5th frame in the range 100-500).  Setting `frames` will override the num_frames variable selection.  The latter option can be useful if you have a baseline part of the video without the animal that you would like to use to generate the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "reference, image = lt.Reference(video_dict,stretch,crop=crop,num_frames=100,frames=None) \n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - User specifies video of empty box\n",
    "The following code allows the user to specify a different file.  Set video_dict['altfile'] to the alternative filename.  Notably, an average is still taken of multiple frames.  The number of random frames used can be changed with the variable `num_frames`.  Alternatively, if the user would like to manually define the frames to be used, `frames` can be set to a list frame numbers: e.g. `frames = np.arange(100,500,5`, which would select every 5th frame in the range 100-500). Setting `frames` will override the num_frames variable selection.  \n",
    "\n",
    "***Defining `frames` is necessary if the alternative video is a different length than the reference video.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "video_dict['altfile'] = 'EmptyBox.avi' \n",
    "reference,image = lt.Reference(video_dict,stretch,crop=crop,num_frames=100,altfile=True,frames=None) \n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. (Optional) Use Interactive Plot to Define Regions of Interest.  \n",
    "\n",
    "After running cell below, draw regions of interest on presented image in the order you provided them.  To start drawing a region, double click on image.  Single click to add a vertex.  Double click to close polygon.  If you mess up it's easiest to re-run cell.\n",
    "\n",
    "***Note*** that there are no problems if regions overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100 \n",
    "\n",
    "plot,poly_stream = lt.ROI_plot(reference,region_names,stretch)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. (Optional) Define Scale for Distance Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Select two points of known distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running cell below, click on any two points and the distance between them, in pixel units, will be presented/returned. Will be used to convert pixel distance to other scale. Note that once drawn, points can be dragged or you can click again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 100\n",
    "\n",
    "dist_plot, dist = lt.DistanceTool(reference,stretch)\n",
    "dist_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Define real-world distance between points\n",
    "Below, set the distance between the points selected below, and the scale. Note that scale can be any desired text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = {\n",
    "    'distance' : 100,\n",
    "    'scale' : 'cm' \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Track Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Set Location Tracking Parameters\n",
    "Location tracking examines the deviance of each frame in a video from the reference frame on a pixel by pixel basis.  For each frame it calculates the center of mass for these differences (COM) to define the center of the animal.  \n",
    "\n",
    "In order to improve accuracy, the parameter loc_thresh is used to remove the influence of pixels that are minimally different from the reference frame.  For each frame relative to the reference frame, the distribution of difference values across pixels is examined and only values above loc_thresh percentile are considered.  We have been using 99-99.5 and this works well.  Values can range from 0-100 and floating point numbers are accepted.\n",
    "\n",
    "The parameters: use_window, window_size, and window_weight are employed to reduce the chance that any objects other than the animal  that might enter the frame (e.g. the hand of the experimenter) influence location tracking.  For each frame, a square window with the animal's position on the prior frame at its center is given more weight when searching for it's location (because an animal presumably can't move far in a fraction of a second).  When window_weight is set to 1, pixels outside of the window are not considered at all; at 0, they are given equal weight.  Notably, setting a high value that is still not equal to 1 (e.g. 0.9) should allow the program to more rapidly find the animal if by chance it moves out of the window.\n",
    "\n",
    "The paramater method accepts values of 'abs', 'light', and 'dark'. 'abs' does not take into consideration whether the animal is lighter or darker than the background and will therefore track the animal across a wide range of backgrounds. 'light' assumes the animal is lighter than the background, and 'dark' assume the animal is darker than the background. 'abs' generally works well, but there are situations in which you may wish to use the others.\n",
    "\n",
    "Lastly, rmv_wire can be used to implement a procedure for removing the influence of wires affixed to an animal.  Set to either True or False.  When using rmv_wire, wire_krn must be set.  This sets the size of an image processing kernel to remove the wire, and should be no larger than the minimum width of the animal, in pixels.  The impact of modifyingn this parameter can be viewed in 8b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_params = {\n",
    "    'loc_thresh' : 99, \n",
    "    'use_window' : True, \n",
    "    'window_size' : 100, \n",
    "    'window_weight' : .9, \n",
    "    'method' : 'abs',\n",
    "    'rmv_wire' : True, \n",
    "    'wire_krn' : 10 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Display Examples of Location Tracking to Confirm Threshold\n",
    "In order to confirm threshold is working, a subset of images is analyzed and displayed using the selected tracking_params.  The original image is displayed on top and the difference values are presented below.  The center of mass (COM) is pinpointed on images.  Notably, because individual frames are used, window settings are not applicable here.  Because of this, actual tracking in video is likely to be better.\n",
    "\n",
    "The user can change the number examples below as they see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size = 100\n",
    "\n",
    "examples = 4 \n",
    "images=lt.LocationThresh_View(video_dict,reference,tracking_params,crop=crop,examples=examples,stretch=stretch)\n",
    "images.cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Track Location and Save Results to .csv File\n",
    "For each frame the location of the animal's center of mass is recorded in x/y coordinates.  If ROIs are supplied, for each frame it is determined whether the animal is in each of the ROIs.  Frame-by-frame distance is also calculated in pixel units.  This data is returned in a Pandas dataframe with columns: frame, x, y, dist, and whether the animal is in each ROI specified (True/False).  Data is saved to a .csv in the same folder as the video.  First 5 rows of data are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=lt.TrackLocation(video_dict,tracking_params,reference,crop=crop)\n",
    "if region_names != None: \n",
    "    location = lt.ROI_Location(reference,location,region_names,poly_stream) \n",
    "if 'scale_dict' in locals():\n",
    "    location = lt.ScaleDistance(scale_dict, dist, df=location, column='Distance_px')\n",
    "    \n",
    "location.to_csv(os.path.splitext(video_dict['fpath'])[0] + '_LocationOutput.csv')\n",
    "location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8d. Display Animal Distance/Location Across Session\n",
    "Below, the animals distance and location across the video is plotted.  Smooth traces are expected in the case where the animal is tracked consistently across the session.  Under heatmap, sigma controls 'binning' of location. When 'sigma = None' default value is provided; but sigma can also be set to any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "\n",
    "#Plot Distance Across Session\n",
    "w, h = 600,200 #specify width and height of plot\n",
    "dist_plot = hv.Curve((location['Frame'],location['Distance_px']),'Frame','Pixel Distance').opts(\n",
    "    height=h,width=w,color='red',title=\"Distance Across Session\",toolbar=\"below\")\n",
    "\n",
    "#Plot Trace of Animal Across Session and Generate Heatmap\n",
    "tracks = lt.showtrace(reference,location,color=\"red\",alpha=.05,size=2,stretch=stretch)\n",
    "heatmap = lt.Heatmap(reference, location, sigma=None,stretch=stretch)\n",
    "(tracks+heatmap+dist_plot).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. (Optional) Create Binned Summary Report and Save\n",
    "The code below allows the user to either save a csv containing summary data for user-defined bins (e.g. proportion of time in each region and distance travelled for each minute) or a session-wide average. \n",
    "\n",
    "***If you only want a session avg***, set `bin_dict = None`\n",
    "\n",
    "***If you are not using ROIs***, in the code below, set value of 'region_names' within function to None: `region_names=None`.  Otherwise, keep `region_names=region_names`\n",
    "\n",
    "To specify bins, set bin_dict using the following notation, where start and stop represent time ***in frames***:\n",
    "\n",
    "```\n",
    "bin_dict = {\n",
    "    'BinName1': (start, stop),\n",
    "    'BinName2': (start, stop),\n",
    "    'BinName3': (start, stop),\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Wanna get fancy?  Check out how to use Python dictionary comprehensions to make 20, 30 second bins, using 1 line of code:\n",
    "\n",
    "`bin_dict = {x:(x*30*60,(x+1)*30*60) for x in range(20)}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_dict = {\n",
    "    '1':(0,10),\n",
    "    '2':(10,20),\n",
    "    '3':(20,30)\n",
    "}\n",
    "\n",
    "summary = lt.Summarize_Location(location, video_dict, bin_dict=bin_dict, region_names=region_names)\n",
    "summary = lt.ScaleDistance(scale_dict,dist,df=summary,column='Distance_px') if 'scale_dict' in locals() else summary\n",
    "summary.to_csv(os.path.splitext(video_dict['fpath'])[0] + '_SummaryStats.csv')\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. (Optional) View Video of Tracking\n",
    "**Note** that tracking must be done before this (Step 7c). \n",
    "The code below allows the user to play back a section of the analyzed video, with the center of mass of the animal marked, to confirm tracking.  User defines the start and end frames of the section they would like to watch.  Of note, if the user sets the start frame for location tracking to something other than 0 in Step 2, the frame start and end here will be relative to that. If the displayed size of the video should be changed, the user can set the `resize` key below to a tuple with the desired width and height, in pixel units... for example: `'resize' : (width,height)`.  The specified width and height should be integers. Lastly, the video can be saved by setting `'save_video' : True`. `fps` controls speed at which frames are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=100\n",
    "display_dict = {\n",
    "    'start' : 500, \n",
    "    'stop' : 560, \n",
    "    'fps' : 30,\n",
    "    'resize' : None,\n",
    "    'save_video' : False\n",
    "}\n",
    "\n",
    "lt.PlayVideo(video_dict,display_dict,location,crop=crop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
